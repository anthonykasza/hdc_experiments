An attempt to learn more about VSAs and HDC.


References
----------
- Language Geometry using Random Indexing
- Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors (Kanerva)
- Holographic Reduced Representations (Plate)
- HDCluster: An Accurate Clustering Using Brain-Inspired High-Dimensional Computing
- A comparison of vector symbolic architectures
- [Computing with High-Dimensional Vectors](https://www.youtube.com/watch?v=zUCoxhExe0o) (Kanerva) 
  - Stanford University Colloquium on Computer Systems EE380 Spring 2023
- Learning with Holographic Reduced Representations
- [Vector Symbolic Architectures In Clojure](https://www.youtube.com/watch?v=j7ygjfbBJD0) (Carin Meier)
- Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens
- GraphHD: Efficient graph classification using hyperdimensional computing
- [Understanding Hyperdimensional Computing for Parallel Single-Pass Learning](https://github.com/Cornell-RelaxML/Hyperdimensional-Computing)
- A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations, A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges
- Hyper-Dimensional Computing Challenges and Opportunities for AI Applications
- SearcHD: A Memory-Centric Hyperdimensional Computing with Stochastic Training
- Classification using Hyperdimensional Computing: A Review
  - table 1 is interesting
- Hyperdimensional Biosignal Processing: A Case Study for EMG-based Hand Gesture Recognition
- HYPERDIMENSIONAL COMPUTING: A FAST, ROBUST AND INTERPRETABLE PARADIGM FOR BIOLOGICAL DATA
  - figure 1b is awesome! and is similar to Figure 1 of Modular Composite Representation
- Modular Composite Representation (MCR)
  - Thank you to the University of Memphis for making the research paper EASY to locate on the interwebs and FREE, as ,in beer, to download
    - hurray! open science!
  - figure 4 reminds me of Gilbert Strang's [The Big Picture of Linear Algebra](https://youtu.be/rwLOfdfc4dw?t=284)
    - orthogonality around a circle
- Hyperdimensional Hashing: A Robust and Efficient Dynamic Hash Table
- HDTest: Differential Fuzz Testing of Brain-Inspired Hyperdimensional Computing
- [VSAONLINE](https://sites.google.com/view/hdvsaonline/home) and its [GGroup](https://groups.google.com/g/vsacommunity/about)
- [MIDNIGHTVSA](https://sites.google.com/ltu.se/midnightvsa)
- [Vector Symbolic Architectures](https://video.ucdavis.edu/media/Vector+Symbolic+Architectures/1_9b6hn4p2) (Luis El Srouji)
- Classification and Recall With Binary Hyperdimensional Computing: Tradeoffs in Choice of Density and Mapping Characteristics
- Robust Hyperdimensional Computing Against Cyber Atacks and Hardware Errors: A Survey
- EventHD: Robust and efficient hyperdimensional learning with neuromorphic sensor
- [Get to know SAR, Interferometry](https://nisar.jpl.nasa.gov/mission/get-to-know-sar/interferometry/)
- Generalized Holographic Reduced Representations
- Recasting Self-Attention with Holographic Reduced Representations
- Deploying Convolutional Networks on Untrusted Platforms Using 2D Holographic Reduced Representations
- [Neuroscience 299: Computing with High-Dimensional Vectors - Fall 2021](https://redwood.berkeley.edu/courses/computing-with-high-dimensional-vectors/)
- Fractional Binding in Vector Symbolic Architectures as Quasi-Probability Statements
- [HDC/VSA: Binary Sparse Distributed Representation with segments](https://github.com/benjamin-asdf/vsa-binary-sparse-distributed-segments-clj)
- [Learning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception](https://ece.umd.edu/release/helping-robots-remember-hyperdimensional-computing-theory-could-change-the-way-ai-works)
- Holographic Global Convolutional Networks for Long-Range Prediction Tasks in Malware Detection
- Configurable Hardware Acceleration for Hyperdimensional Computing Extension on RISC-V
  - hyperdimensional coprocessor unit - push HDC tasks into hardware
- Orthogonal Matrices for MBAT Vector Symbolic Architectures, and a “Soft” VSA Representation for JSON
  - represent JSON objects as hypervectors for ML on JSON. very cool.
    - i wonder if nosql databases do something like this
  - bindings can be typed by introducing a common signal (a random matrix) in grouped symbols, this is how nested structures can be created
  - "VSAs in general, can be viewed as similarity-preserving hash codes for complex structures, where resulting hashes are amenable to machine learning and to searching for nearest neighbors"
- Efficient Host Intrusion Detection using Hyperdimensional Computing
  - they can find attack in Sysflow data by embedding provence graphs and ATT&CK TTPs into hypervectors and then mathing on them
  - "We adopted the method to work with an arbitrary length of paths within the graph, enhancing the capability of our framework to efficiently identify attack patterns of any length within provenance graphs"
- Audio Fingerprinting with Holographic Reduced Representations
- HyperCam: Low-Power Onboard Computer Vision for IoT Cameras
  - HDC vision using COTS hardware. bravo!
    - DVS are neat but they aren't cheap
  - naive encoding method
    - pixel position codebook uses randomized leveling
      - there will be no correlation between levels?
    - pixel value codebook uses linear leveling
      - the paper randomizes which bits they flip but all the bits hold the same amount of information so it doesn't matter
    - pixels are encoded as bind(Row, Col, Val)
    - images are encoded as bundle(pixel_binding0, pixel_binding1, ...)
  - rewrite 1
    - utilize permutation operation instead of position codebook to save memory
      - still no correlation between row1,col1 and row2,col1?
  - rewrite 2
    - another codebook reduction to save memory
    - pixels are treated as 1d?
  - rewrite 3
    - introduced weighted bundling operation to ensure pixel values that occur more often are considered more important
      - this sort of reminds me of adjusting the contrast on a b/w image
        - up the black and white values so the mid-tones become less important
  - rewrite 4
    - the sparse bundling operation:
      - randomly select some percent of the elements and bundle those only
      - use a CountSketch or BloomFilter backend
    - they merged the binding operation and generation of value HVs into a single pass for more better performance
  - remaining thoughts
    - i don't understand how BF and CS are used
    - i really like the idea of a partial bundling operation
      - it seems related to leveling strategies (replace, average, inc/dev)
      - how would other other operation modifications be useful? partial permutation?
    - they are very concerned with performance. why not reduce HV dimensions? this would have made all operations more efficient (and less accurate)
    - they don't address the fact that neighboring pixels are often very near in value
- Detecting COVID-19 Related Pneumonia on CT Scans using Hyperdimensional Computing
  - this is awesome because it's theoretical AND tangible
    - wow, expert radiologists are only 70% accurate!
  - the use of opencv to adjust contrast prior to encoding reminds me of HyperCam's weighting added in rewrite 3
    - they use b/w images and pixel values of 0-255
  - "For creating the hypervectors, we use orthogonal or uncorrelated encoding [13]–[15] to represent the position of each pixel and linear or correlated encoding [17] to represent the pixel intensity."
    - similar to HyperCam, this paper uses uncorrelated positional encoding of pixels
    - using orthogonal encoding for pixels reminds me of using categorical encoding for integer values
    - dataset 3 having different image sizes than datasets 1 and 2 complicates things
      - i dunno if this will work but... use a single codebook of size 512 (the largest feature range) for all features
        - for 200x300 pixel images, flip some percent of the HV's elements to 0
          - `x_hv = zero_flip(levels[pixel_x], 312/512)`
          - `y_hv = zero_flip(levels[pixel_y], 212/512)`
          - `value_hv = zero_flip(levels[pixel_val], 256/512)`
        - for 512x512 pixel images, don't flip anything for positional variables
          - `x_hv = levels[pixel_x]`
          - `y_hv = levels[pixel_y]`
          - `value_hv = zero_flip(levels[pixel_val], 256/512)`
        - then `pixel_binding0 = bind(x_hv, y_hv, value_hv)`
        - then `image_bundle = bundle(pixel_binding0, pixel_binding1 ... pixel_bindingN)`
  - "Future research needs to be done to discover an encoding that is not dependent upon pixel position and that could be implemented to three-dimensional images"
- [A Brain-Inspired Hyperdimensional Computing Approach for Classifying Massive DNA Methylation Data of Cancer](https://github.com/cumbof/chopin2)
  - they, too, use uncorrelated levels to encode a range of numberical values. why?
    - i tried MNIST classification with leveled codebooks and it didn't work as well as randomized codebooks. i don't understand why
  - this is very cool: during training, if the wrong class is predicted, they subtract the sample_hv from all incorrect class prototypes
    - then they continually bundle the sample_hv into the correct class prototype HV until the accuracy reaches some threshold 
  - they also demonstrate that more dimensions and more levels doesn't mean better accuracy. table 4 shows that there's a sweet-spot for the datasets they use
- HDC-MiniROCKET: Explicit Time Encoding in Time Series Classification with Hyperdimensional Computing
  - i don't understand most of this paper but...
  - they bind in the timestep of the observation when encoding, which validates what thingy/ does
    - they use fractional binding (power encoding) to level the timesteps
      - they also mention DFT so i assume they're using FHRR
    - "To be able of adjust the temporal similarity of feature vectors, we introduce a parameter s, which influences the graded similarity between consecutive timestamps"
    - "s weigths the importance of temporal position – the higher its value, the more dissimilar become timestamps"
    - "the similarity of two feature vectors, each bound to the encoding of their timestamp, gradually decreases with increasing difference of the timestamps."
    - "It is very important to keep in mind that not all tasks and datasets benefit from explicit temporal encoding (e.g.. we can similarly construct datasets where temporal encoding is harmful."
    - "In practice, selecting s should incorporate knowledge about the particular task."
- [MIT 9.13 The Human Brain, Spring 2019](https://www.youtube.com/watch?v=ba-HMvDn_vU&list=PLUl4u3cNGP60IKRN_pFptIBxeiMc0MCJP)
  - this course is radical
  - grid cells, whoa
- Intrusion Detection in IoT Networks Using Hyperdimensional Computing: A Case Study on the NSL-KDD Dataset
  - i like the application of HDC to network detection <3
  - i don't like using public intrusion datasets
  - i don't like assuming DoS or scan detection has any relevance to other types of attacks
  - "Another avenue for future research could involve extending the model to handle sophisticated attacks, such as zero-day and advanced persistent threats, and incorporating real-time detection and response for better performance in dynamic, resource-constrained IoT environments."
    - let's say you bought 100 zero-days to further evaluate this experiment
      - at $100,000 each, that'll only run about $10,000,000
      - once you buy an ohdays, it is an ohday no longer
- Network Anomaly Detection for IoT Using Hyperdimensional Computing on NSL-KDD
  - this is very similar to "Intrusion Detection in IoT Networks Using Hyperdimensional Computing: A Case Study on the NSL-KDD Dataset" but with 1/4 different authors and way more math equations
- An Extension to Basis-Hypervectors for Learning from Circular Data in Hyperdimensional Computing
  - basis-hypervectors, a representation of an atomic symbol
  - level-hypervectors, derived from, and correlated with, a basis-hv
  - class-hypervectors, a centroid or "prototype"
  - query-hypervectors, an unlabeled HV which is queried against all class-hvs
  - classification model, a group of class-hvs is the model. compare a query-hv against each of the class-hvs to determine its label
  - regression model, a set of linearly leveled level-hvs is the model. 
    - starts at a place and goes to another place
    - requires a lookup function to conver the real-number to an HV
    - bundle the levels to make a levels_bundle, then bind(query, levels_bundle)
      - i'm pretty sure this is what factorization.py does
  - on uncorrelated codebooks "While this seems suitable for encoding letters, which to some extent represent unrelated information, clearly it is not as adequate for other kinds of unitary information, such as real numbers"
  - "the level-hypervectors created with the existing method, as described above, have a fixed distance between each pair of hypervectors."
  - circular-hypervectors, similar to leveling but each "level" hv represents a single point "of a set of equidistant points on a circle"
    - both even and odd number of levels can be used
    - hvs which are represent opposite side of the circle have minimum similarity (they are as similar are any 2 random hv)
    - see figure 6
  - their results
    - surgery robots, so cool
    - it's nice to see that leveling didn't perform as well as random symbols for them too on a classification task. maybe that's why it doesn't work well on MNIST digit classification
  - is it possible to create exponential/log correlate levels?

  - why haven't any of the paper's on image classification use 2d ngrams?
    - random (non) levels
    - linear levels
    - circular levels
    - permutation
    - no one has tried using column and row ngrams. sorta like a kernel/mask/convolution
      - after using photoshop filters for last 20 years i finally understand how they work
- [Fractional Binding in Vector Symbolic Architectures as Quasi-Probability Statements](https://www.youtube.com/watch?v=aYbJ_beUja8)
  - VSAs enable probabilist programming
    - binding encodes data, similarity computes probability, bundling updates beliefs (learns), unbinding is sort of like conditional probabilities
    - lots o' math
      - "Fractional binding is mathematically equivalent to the inverse Fourier transform of data encoded with RFFs"
    - let's make some mental leaps: VSAs are probabilities, quantum things are probabilities, VSAs are quantum things, VSAs are neuromorphically inspired, our brains are quantum computers
  - why do the waterloo papers use a different format for their References sections compared to other papers?
- [Efficient navigation using a scalable, biologically inspired spatial representation](https://www.youtube.com/watch?v=QrvUVECQDkk)
  - representing physical spaces with VSAs, they call them SSP (spacial semantic pointer)
  - grid cells, whoa
  - wow! tiled mazes
  - [code](https://github.com/ctn-waterloo/cogsci2020-ssp-nav). thank you.
- A neural representation of continuous space using fractional binding
  - semantic pointers, i don't fully understand them




Summary
-------
What is Hyperdimensional computing?
See a [Tutorial on Hyperdimensional Computing](https://michielstock.github.io/posts/2022/2022-10-04-HDVtutorial/).

What makes a Vector Symbolic Architecture?
- concepts are high, but fixed, dimension random vectors. structure is built up from randomness
- holographic elements / distributed information
- operatorations: multiply, add, permute, similarity, ...

What are the differences between VSAs?
- Denis Kleyko provides a great [Overview of different HD Computing/VSA models](https://redwood.berkeley.edu/wp-content/uploads/2021/08/Module2_VSA_models_slides.pdf)
- [A comparison of Vector Symbolic Architectures](https://arxiv.org/abs/2001.11797) provides a comprehensive taxonomy of architectures

Why use HDC?
- HDC is inspired by the Tensor Product Representation
- supports probabilistic calculations
- supports online/streaming learning
- it aligns well with the theory of "distributed representation" aka "assemblies of neurons" theory of the brain
- learned results are not a blackbox but are instead interpretable
- by pushing most of the heavy computations into embedding, the compelxities of learning are reduced


Notable VSAs
------------
Holographic Reduced Representations
- "reduced", all HVs are fixed length
- "holographic", all elements represent information equally
  - a subset of bits from an HV represents the same object, just with less precision
    - 10 randomly selected bits from the HV represents the same symbol as all 10_000 bits
    - this is akin to cutting a hologram into pieces
  - what is a hologram?
    - jó napot, Gabor úr
    - holograms involve lasers and lightwave interference patterns
      - scanning objects with interference patterns is called interferometry
        - interferometry has a ton of applications, e.g. JPL used it to measure surface topography changes after the 2014 Napa earthquake
- fourier HHR
  - FHRR/HRRF is measurably better than other VSAs in some cases
  - i think this is because the more information a single element can hold (the more complex the number) the more effective the VSA can be
    - the more complex the element type the greater the hardware requirements
  - each element of a HV is a random phase angle (phasor) between -pi and pi
  - magnitude only is used
    - this appears related to spiking networks architectures

Sparse Block Codes, Binary and Generic
- HV is partitioned into blocks (segments) of equal size 
  - the HV’s dimensionality is a multiple of the block size
- block-wise (segment-wise) operations
  - ensure a specified sparsity
  - permute the block
  - combine blocks with other blocks or scalars
    - bind, bundle, substitue, maybe further subdivide the block?
      - block of block codes, hyperdimensional blocks

Bloom filters, a special case of VSA
- a set is represented by a binary vector
  - an empty set is all zeros
  - a single vector is more memory efficient than storing all samples
- when adding an element to a bf, the item is hashed with several functions
  - the functions result in an index which is flipped from 0 to 1
- when checking inness, an element is hashed (using the same set of functions)
  - the resulting indices of the bit vector are then checked for 1 values
  - if indices are 0, the item is definitely not in the set
  - hash collisions may cause FPs
- no FNs, possible FPs
  - is this thing in your cache? the bf can answer with 'definitely no' or 'maybe yeah'
- what happens if we were to introduce noise and flip a few random bits in a vector?
  - what happens to a bloom filter?
    - FPs introduced for 0 bits changed to 1 bits
    - FNs introduced for 1 bits changed to 0 bits
  - what happens to the similarity between two HVs?
    - not much



VSA Operations 
--------------
not all operations are applicable to all architectures.
operations can be conceptualize with 3 abstraction levels:
- elements operations which result in blocks or vectors
- blocks operations which result in blocks or vectors
- vectors operations which result in vectors or vector spaces

implementations of operations need to consider:
- atomic elements
  - types: bool, int, float
  - algebraic qualities: identity, symmetry, inverse
  - bounds checking
    - clipping, ensure element values are within a range
    - normalization, ensure element values are normalized to a range
- dimensionality and segmentation of vectors
- sparsity of vectors or segments
  - adding sparse vectors decreases sparsity
  - multiplying sparse vectors increase sparsity
- precision of results
  - cleanup step

operations include:
- addition, summing two vectors into a single vector preserves information from both consituents
  - aka: bundle, summation, superposition, learning, accumulation, majority vote

- multiplication, multiplying two vectors into a single vector moves the relationship between the inputs to a new region of the hyperspace 
  - aka: bind, compose, XOR, FFT, circular convolution
    - binding approximates TPR by because HDC requires fixed-dimensionality
    - see The Binding Problem
  - exponentiation
    - fractional power encoding (FPE)
      - aka: trajectory association
      - bind vector x times with itself, then the vector represents x
        - raise each element to the exponent x
        - this only works if the bind op is NOT the inverse of itself
      - multiplying is the same as adding exponents if the base vectors are the same
        - accomplishes scalar-like behavior 
      - variants on FPE
        - FPE with hadamard binding (phasor)
        - FPE with circular convolution binding (real valued)
        - block local circular convolution (sparse)
        - VFA, vector function architecture
          - FPE VSA plus a kernel function
            - your task will dictate your kernel but it opens the door to using VSA for learning with kernel functions
              - multidimensional kernels
              - window/modulus/circular kernels
              - periodic multidimensional kernels
                - grid cells, hex pattern in mice neurons
                - crystallography
                - lattice-based crypto
          - "the distribution from which components of the base vector are sampled [how sparsity is sprinkled into the HVs] determines the shape of the FPE kernel, which in turn induces a VFA for computing with band-limited functions"
          - any FPE with uniformly sampled base vectors have a universal kernel
            - whittaker-shannon interpolation formula
              - sinc function
                - normalized vs not
                - well defined envelop
                - crosses zero at the integers
          - binding a scalar to a vector (function) shifts the vector
          - binding 2 vectors (functions) together is a convolution of functions
             - functions are compositional
          - calculate similarities between functions

- division, undo multiplication
  - aka: unbind, factorization, decomposition
  - in some VSA unbinding is imperfect and requires a second cleanup operation

- cleanup, replace an operation's result with something else based on the result
  - its nearest neighbor in memory
    - resonator networks
    - replace the noisy HV from unbinding with the most similar HV in memory
  - a filtered version of itself
    - thinning
      - ensure the density/sparsity of a vector/segment

- permutation, preserves the order of elements or segments 
  - aka: shift, rotate, braid, protect
  - the operation needs to be invertible
    - does not need to be perfectly invertible if a cleanup is used
  - permute is similar to multiple
  - reverse is one type of permutation operation which:
    - takes 0 parameters
    - is lossess
    - is the inverse of itself
  - shift is one type of permuation operation which:
    - takes 1 parameter
    - is lossess
    - can be inverted by flipping the sign of the parameter

- similarity, a measure applied to vectors (segments) pairwise
  - e.g. cos similarity, hamming distance
  - similarity is robust to noise

- substitution, mutating an HV to become more similar to another HV
  - this is useful for leveling a vector space
  - HV1 becomes more similar to HV2, HV2 remains unchanged
  - a sequence of 'levels' (bins/buckets) is produces which leads from HV1 to HV2

- segmentation, create structure or depth
  - segment a vector into positional blocks
    - aka: blocking, grouping, chunking, windowing
    - locality preserving encoding (LPE)
      - thermometer code
        - linearly discretized levels
          - the first vector is hdv(all=0)
          - the last vector is hdv(all=1)
          - the HV grows its count of 1 values by flipping 0s to 1s by incrementing index
        - bundle(HVs[2],HVs[2]) != bundle(HVs[1],HVs[3])
          - if using integers: 2 + 2 != 1 + 3
          - consider the linearly discritized vector of hypervectors: HVs
          - bundling/binding indices of the hyperspace do not behave as adding/multiplying integers would
      - float code / sliding code
        - simmilar to 1-hot but more like window-hot, where the window is centered around the element
        - uses a fixed width window, slide across the all zeros vector, ensure bits in the window are 1s
        - the window is slid across the all-zeros HV
        - the start of the window is the HV's index in hypserspace
        - more sparse compared to thermometer codes
          - different similarity kernel
      - scatter code
        - no strict limitation on number of levels in a hyperspace
        - hspace[0] = some random dense vector
        - each level is created by randomly flipping a few elements in the previous unit

  - segment the 'space' between vectors into levels
    - aka: leveling, sampling, binning, discretization, quantization, bucketing
    - enearby levels are somewhat similar, distant levels are dissimilar
    - leveling strategies
      - linear, for representing a continuous range as an evenly spaced buckets
        - exact linear, dimensions / bins = elementsPerBin
        - approximate linear mapping - cheaper than exact linear mapping 
          - "Approximate linear mapping [58] does not guarantee ideal linear characteristic of the mapping, but the overall decay of similarity between feature levels will be approximately linear"
          - only store the start and stop HVs instead of the entire hyperspace
          - construct levels on the fly
      - circular, useful for modulus/cyclical calculations such as:
        - seasons of the year, hours of the day, months of the year, color spaces, round-robin hashing (rendezvous/hrw)
      - logarithmic/exponential, shrink of shrink/growth of growth
      - fibonacci (retracement)
      - combine different leveling strategies to create a vector space with varying granularity
        - e.g. log then linear
        - elliptic, like a circle but longer on two of the sides



Searching Memory
----------------
Consider the following: unbinding a "scene" of objects each with some set of properties
- decompose the scene into its composed constituents
  - objects in the scene have properties
    - color
    - shape
    - location
      - x
      - y
  - e.g. scene = bind(HV1, HV2, HV3)
    - HV1 = bind(color_HV, shape_HV, bind(x, y))
    - HV2 = bind(color_HV, shape_HV, bind(x, y))
    - HV3 = bind(color_HV, shape_HV, bind(x, y))
  - to understand the scene, you need to search the codebook for combinations of all atomic symbols for all properties
    - this can be expensive, which is why selecting an efficient encoding method is important
  - assume there are 100 unique items in each HV's codebook (lookup memory)
    - 100 possible items for HV1
    - 100 possible items for HV2
    - 100 possible items for HV3
    - worst case: 10 * 100 * 100 = 1_000_000 item combinations to check cossim with
- Resonator Networks
  - aka Hopfield Network
  - similar to the result of gradient descent
  - an iterative algorithm that searches the combinatoric space of the codebooks without searching by exhaustion
  - given an HV from a binding operation, the codebooks for the input of the binding operation, determine the inputs of binding operation
    - create 'estimate' vectors, these represent your initial guesses
      - xhat, yhat, zhat = hdv(), hdv(), hdv()
    - do something with the estimates
    - update the estimates based on results of comparison to portions of the codebooks
    - utilizes superimposed 'guesses' or 'estimates' to find best guesses for one parameter at a time
    - iterates to find best
      - z, with estimates of y and x
      - y, with estimates of x and z
      - z, with estimates of y and z
- Hyperdimensional Quantum Factorization
  - in archetectures where unbinding is noisy (bind is not the exact inverse of unbind) a cleanup step is used
  - this paper utilizes Grover's algo to speed up the memory search done in the cleanup step
    - this approach is better than resonator networks
  - hardware does not currently exist to implement. womp womp.


Composing Structure with Vectors
--------------------------------
- sets (bundle)
- maps (bind)
- sequences (permute)
  - graphs (bind nodes, bundle paths)
    - paths and words are both ngrams
      - bind(HV1, perm(HV2, 2), perm(HV3, 3)
    - trees
    - finite state automata
- stacks

Misc
----
- torchhd and hrr python modules
- HDC accuracy can be improved by increasing vector lengths (dimensions) or making elements types more complex
  - increasing complexity of each element is "better" at conveying information than making the vectors longer
  - more complex element types make for more complex hardware needs
- hrrformer
- datasets mentioned in literature
  - isolet
  - ucihar
  - mnist
  - UCI Machine Learning Repository
  - UCR Time Series Archive
  - Numenta Anomaly Benchmark (NAB)
  - Long-Range Arena (LRA)
  - malware
    - Microsoft Malware Classification Challenge (Kaggle)
    - The Drebin Dataset
    - EMBER (Elastic Malware Benchmark for Empowering Researchers)
  - eTraM : Event-based Traffic Monitoring Dataset
    - FREE event data from a DVS camera
    - https://github.com/eventbasedvision/eTraM
    - https://eventbasedvision.github.io/eTraM/
  - [CICIDS2017](https://www.unb.ca/cic/datasets/ids-2017.html)
  - [Car-Hacking](https://ocslab.hksecurity.net/Datasets/car-hacking-dataset)
  - NWPU-RESISC45 - REmote Sensing Image Scene Classification
  - FMA: A Dataset For Music Analysis
- VSA has relationships with compressed sensing, which makes sense given how bundling of vectors is how VSA "learns"
- when creating vectors, the distribution of element values does not need to be random (50% 1's and 50% 0's)
  - it may be useful to create sparse vectors where the distribution of 1's is 1% of the elements
  - sparse vectors are more easily compressed, making them more memory efficient 
- how to encode a vector into a vector symbol? multiply it by a constant random matrix (a projection/hat matrix)
- one of the big issues with HDC/VSA is that there is no standard method of encoding the application-specific data into vectors
  - should i bind with multiplication or permuation? that depends on your use-case.
  - "the HV representations must be designed to capture the information that is important for solving the problem and presenting it in a form that can be exploited by the HDC/VSA"
  - 2d images need special encoding steps to ensure nearby pixels are "related" to each other
    - turning a 2d image into a 1d vector simply by concatination is naive
    - there is a need to incorporate both x and y axis data as well as pixel color value
    - partial permutation can address this by creating a radius where similar colors will have similar HVs
    - fractional power encoding can also be used
      - generate role-filler HVs for x and y
      - then raise the x vector to the exponent indicating the column of the pixel
      - do the same for the y
      - bind the pixel value HV with the exponentialized x and y HVs
- HDC can be incorporated into NN to make both better
  - NN frontend to generate HVs
  - HDC frontend to generate vectors for NN
- what happens when a HDC model is trained on "levels" but then tested with samples that are outside of the levels' range's max/min?
- fix sized ternary vectors remind me of
  - nPrint
    - concatenate all maximum length PDUs together to make a long sparse block vector
  - Ramanujan's sum, namaste sir
    - infinite sum of natural numbers equals -1/12
    - one of the steps to solve the formula is to calculate 1/2 as the sum of the infinite series: 1 - 1 + 1 - 1 + 1 - ...
- multiple time-based signals can be quantized, then the signal at each timestep can be bundled together
  - combining multiple signals into a single vector
  - this was done for seizure detection as well as gesture identification
- "In general, DL’s [Deep Learning] strength is in learning a mapping from one space to another, given that these spaces are densely populated with examples. HDC, however, shines when there is a specific, known structure that one wishes to encode."
- HDC models can be improved with adversarial mutated samples, just like other models
  - mutate/alter the training data with some strategy (random noise, column/row permuation, etc)
  - train/test on the mutated data
  - inspired by fuzzing techniques
- HDC has capacity limits in the number of symbols...
  - you can have in working memory given the need for a cleanup step in retrieval
  - you can bundle together before the resulting HV converge to random noise
    - this causes results of bundling to "forget"
- Random High-Dimensional Binary Vectors, Kernel Methods, and Hyperdimensional Computing
  - https://cse.umn.edu/ima/events/random-high-dimensional-binary-vectors-kernel-methods-and-hyperdimensional-computing
  - i do not understand all the math discussed
 - if you're working with spacial data and you don't encode spacial features in your VSA pipeline then the results will not be great
- a block can be thought of and operated on like a 'sampled' (or 'reduced') version of its source vector
  - all HV symbols are conceptually sampled versions of larger, and more precise, vector
    - the tensor product is the Platonic Form
  - blocks are a special case of vector which carry with them a:
    - source vector
    - contextual mapping into their source vector
- there is no least significant bit in a hyper vector. all the elements hold th same amount of information
- concentration of measure ensures randomly intialized HVs are dissimilar
- each dimension (element) of an HV increases the vector space exponentially
  - projecting something onto a bigger surface is often useful
    - think about a screen projector, it makes small images easier to see
- [zotero HDC/VSA group library](https://www.zotero.org/groups/5100301/hdvsa_literature/library)
- if we can substitute from one HV to another, making hv1 more similar to hv2...
  - can we make hv1 more unlike hv2?
  - instead of copying bits from hv2 into new levels, copy flipped bits from hv2 into hv1
- i wonder if our brains bundle when we sleep
- [thanks](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Memorial_to_the_lab_animals_%2814604111622%29.jpg/1024px-Memorial_to_the_lab_animals_%2814604111622%29.jpg)
